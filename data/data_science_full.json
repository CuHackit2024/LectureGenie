{"segments": [{"start": "2.309", "end": "14.1", "text": "So, hello everyone and welcome to applied data science today, we're going to do the next big unsupervised learning technique which is called outlier detection.", "frame_description": " The image is a slide from a lecture video. The slide has a dark purple background with a blue and purple gradient at the bottom. The text on the slide is white. The slide is titled \"Outlier Detection & Analysis\". The subtitle is \"Nina Christine Hubig\". There is a small picture of a bridge in the top right corner of the slide."}, {"start": "14.609", "end": "30.51", "text": "Outlay detection is very closely connected to clustering as it is basically the opposite of finding clusters. If a data point is in a cluster, that's not the outlier. And if it's not in any cluster, it's an outlier.", "frame_description": " The image is a slide from a lecture video. The slide has a dark purple background with a blue and purple gradient at the bottom. The text on the slide is white. The slide is titled \"Outlier Detection & Analysis\". The subtitle is \"Professor Nina Christine Hubig\". There are no other notes or diagrams on the slide."}, {"start": "31.479", "end": "36.74", "text": "So what's outline detection and why do we need it", "frame_description": " No notes or diagrams are shown in the image."}, {"start": "37.29", "end": "60.0", "text": "outlier analysis is one of the most famous and most important analysis. We can do think about your credit card if you go to a different country that your credit card checks. If it's a fraud or not, this is called fraud detection. And it's used for credit cards, for telecommunications, any form of criminal activity in, for example, ecommerce", "frame_description": " The image is a slide from a lecture video. The slide is titled \"Motivation for Outlier Analysis\". \n\nThe slide has a dark blue background with light blue text. The logo of the University of South Carolina is in the top right corner. \n\nThe slide has a bulleted list of applications of outlier analysis:\n- Fraud Detection (Credit card, telecommunications, criminal activity in e-Commerce)\n- Customized Marketing (high/low income buying habits)\n- Medical Treatments (unusual responses to various drugs)\n- Analysis of performance statistics (professional athletes)\n- Weather Prediction\n- Financial Applications (loan approval, stock tracking)\n\nThe slide also has a quote: \"One person's noise could be another person's signal.\""}, {"start": "60.74", "end": "73.839", "text": "lay detection is also very much used in customized marketing, high and low income buying habits. What is it that you usually buy, what can be done to give you more to buy", "frame_description": " The image is a slide from a lecture on Applied Data Science. The slide is titled \"Motivation for Outlier Analysis\". It lists several applications of outlier analysis, including fraud detection, customized marketing, medical treatments, analysis of performance statistics, weather prediction, and financial applications. The slide also includes a quote: \"One person's noise could be another person's signal.\""}, {"start": "74.739", "end": "99.309", "text": "medical treatments might be very interesting too. So unusual responses to various drugs, typical outlier detection task and the analysis of performance statistics. For example, in professional athletes, some people behave in an outlier ish way, but also it could be particularly good or particularly bad. That would be an outlier of your standard performance", "frame_description": " The image is a slide from a lecture on outlier analysis. The slide is titled \"Motivation for Outlier Analysis\". \n\nThe slide contains a bulleted list of applications of outlier analysis:\n\n- Fraud Detection (Credit card, telecommunications, criminal activity in e-Commerce)\n- Customized Marketing (high/low income buying habits)\n- Medical Treatments (unusual responses to various drugs)\n- Analysis of performance statistics (professional athletes)\n- Weather Prediction\n- Financial Applications (loan approval, stock tracking)\n\nThe slide also contains a quote: \"One person's noise could be another person's signal.\""}, {"start": "100.379", "end": "129.309", "text": "weather prediction. Everything that is in extreme weather like a hurricane, a tornado, an earthquake, these are outliers, all of them because the most normal weather doesn't have them. So if you look at 40 years of weather, the tornado will be a rare occurrence and with that, it's an outlier. So all weather prediction for such extreme nature catastrophes would be an outlier detection.", "frame_description": " The image is a slide from a lecture video on the topic of \"Motivation for Outlier Analysis\". \n\nThe slide contains the following text:\n\n- \"Fraud Detection (Credit card, telecommunications, criminal activity in e-Commerce)\"\n- \"Customized Marketing (high/low income buying habits)\"\n- \"Medical Treatments (unusual responses to various drugs)\"\n- \"Analysis of performance statistics (professional athletes)\"\n- \"Weather Prediction\"\n- \"Financial Applications (loan approval, stock tracking)\"\n\nThe slide also contains the following diagram:\n\n- A Venn diagram with two circles. \n - The left circle is labeled \"One person's noise\". \n - The right circle is labeled \"Another person's signal\". \n - The intersection of the two circles is labeled \"Outlier\"."}, {"start": "130.839", "end": "148.08", "text": "Um financial applications like loan approvals and stock trading also strongly rely on outlier analysis. So as we say in statistics, one person's noise could be another person's signal. This is the motivation for outlier analysis.", "frame_description": " The image is a slide from a lecture on Applied Data Science. The slide is titled \"Motivation for Outlier Analysis\". It lists the following applications of outlier analysis:\n\n- Fraud Detection (Credit card, telecommunications, criminal activity in e-Commerce)\n- Customized Marketing (high/low income buying habits)\n- Medical Treatments (unusual responses to various drugs)\n- Analysis of performance statistics (professional athletes)\n- Weather Prediction\n- Financial Applications (loan approval, stock tracking)\n\nThe slide also includes the quote \"One person's noise could be another person's signal.\""}, {"start": "149.1", "end": "166.13", "text": "What is an outlier and out any outlier is by definition an observation inconsistent with the rest of the data set that's also called a global outlier. A special outlier. For example, a local outlier is an observation inconsistent with their neighborhood,", "frame_description": ""}, {"start": "166.309", "end": "191.725", "text": "a local instability or discontinuity. So if we look, for example, at um cluster two, this small dense cluster, very dark and cluster C one is more variant. It's a very loosely connected cluster, that's the global data set. And from both these clusters, the global outlier is 01, you see that little point here and the", "frame_description": ""}, {"start": "191.735", "end": "217.16", "text": "local outlier is 02 because it's an outlier in comparison to the density of C two but not as a global one. If you look at the density existing in C one in comparison to C one, this would be the normal distance from its other data points. Nevertheless, because C two is so dense, this 02 is a local outlier", "frame_description": ""}, {"start": "219.0", "end": "242.72", "text": "causes of outliers could be poor data quality or some contamination, low quality measurements, more functioning equipment or manual error. Besides all these kind of error situations, most data sets hold out layers. So this are correct data but exceptions to the existing pattern in the data.", "frame_description": " The image is a slide from a lecture on Applied Data Science. The slide is titled \"Causes of Outliers\". It lists three causes of outliers:\n- Poor data quality / contamination\n- Low quality measurements, malfunctioning equipment, manual error\n- Correct but exceptional data"}, {"start": "242.899", "end": "250.94", "text": "While clustering looks at the patterns, the outlet detection looks at the exceptions. Both of them are necessary and important.", "frame_description": " The image is a slide from a lecture on Applied Data Science. The slide is titled \"Causes of Outliers\". It lists three causes of outliers:\n1. Poor data quality / contamination\n2. Low quality measurements, malfunctioning equipment, manual error\n3. Correct but exceptional data"}, {"start": "252.869", "end": "280.75", "text": "The object objective and outlay detection approaches is to define what data can be considered as inconsistent in a given data set for that. This is why I'm saying they strongly connect on the unsupervised level. You need to know what is consistent in a given data set as we are often more interested in patent motives. In this case, we look for the inconsistency that is not necessarily noisy.", "frame_description": " The image is a slide from a lecture on outlier detection approaches. The slide is titled \"Outlier Detection Approaches\" and it has the following notes:\n\n* Objective:\n    * Define what data can be considered as inconsistent in a given data set\n    * Statistical-Based Outlier Detection\n    * Distance-Based Outlier Detection\n    * Find an efficient method to mine the outliers\n\nThere are no images or diagrams shown in the slide."}, {"start": "281.47", "end": "296.299", "text": "Um We decide we distinguish two different types of outliers, statistic based outlier detection and distance based outline detection. I omitted the one that is in database systems. Um But that would be a third option.", "frame_description": " The image is a slide from a lecture on outlier detection approaches. The slide is titled \"Outlier Detection Approaches\" and has the following notes:\n\n* Objective:\n    * Define what data can be considered as inconsistent in a given data set\n    * Statistical-Based Outlier Detection\n    * Distance-Based Outlier Detection\n    * Find an efficient method to mine the outliers\n\nThe slide also has a diagram of a data set with three outliers. The outliers are shown as red dots, and the rest of the data is shown as blue dots."}, {"start": "296.959", "end": "312.489", "text": "We will find an efficient method to mine these outliers depending on how many features you have. If you have a high dimensionality, a lot and a high amount of variables mining these outliers can be very tricky.", "frame_description": " The image is a slide from a lecture on outlier detection approaches. The slide is titled \"Outlier Detection Approaches\" and has the following content:\n\n- Objective:\n  - Define what data can be considered as inconsistent in a given data set\n  - Statistical-Based Outlier Detection\n  - Distance-Based Outlier Detection\n- Find an efficient method to mine the outliers\n\nThere are no additional notes or diagrams shown in the image."}, {"start": "315.369", "end": "335.26", "text": "Um One, some of, you might ask, well, if we have the pattern, if we know the pattern through clustering, for example, or through some functional algorithm, some function generator, why do we even need a special technique to only identify the outliers?", "frame_description": " The image is a slide from a lecture video. The slide is titled \"Why A Special Technique to Identify Outliers?\". It has a blue background with white text.\n\nThe slide has 4 bullet points:\n- Performance considerations\n- Subjective to the clustering algorithm and clustering parameters\n- Only certain attributes may have outlier properties, no need to disqualify the entire tuple\n- Contamination may occur by \"column\", not by row"}, {"start": "335.929", "end": "357.869", "text": "So what we could instead do, we could find the pattern and then everything that's not in the pattern is the outlier. And of course, these type of methods exist. But there are reasons why you may want to choose a technique that only identifies outliers. And one of the foremost considerations here are performance issues.", "frame_description": " The image is a slide from a lecture video. The slide is titled \"Why a Special Technique to Identify Outliers?\".\n\nThe slide has a blue background with white text. The text is organized into three bullet points. The first bullet point says \"Performance considerations\". The second bullet point says \"Subjective to the clustering algorithm and clustering parameters\". The third bullet point says \"Only certain attributes may have outlier properties, no need to disqualify the entire tuple\".\n\nThere is a small Clemson University logo in the bottom right corner of the slide."}, {"start": "358.39", "end": "377.239", "text": "If you have to understand all the existing patterns in a data set first and name them efficiently, it might be faster to immediately look at the outlier. It's also another reason to subjective to the clustering algorithm, the clustering parameters if they run really slow.", "frame_description": " The image is a slide from a lecture video. The slide is titled \"Why a Special Technique to Identify Outliers?\". It has a blue background with white text.\n\nThe slide has four bullet points:\n- Performance considerations\n- Subjective to the clustering algorithm and clustering parameters\n- Only certain attributes may have outlier properties, no need to disqualify the entire tuple\n- Contamination may occur by \"column\", not by row\n\nThere are no images or diagrams on the slide."}, {"start": "377.779", "end": "402.049", "text": "Um Also we have sometimes only specific attributes that may have interesting outliers, but there is no need to disqualify the entire table. So if I'm looking at the credit card, for example, it's not so important how much money is changed, but it might be important where it is changed. So you look at specific features", "frame_description": " The image is a slide from a lecture video. The slide is titled \"Why a Special Technique to Identify Outliers?\". It has a blue background with white text.\n\nThe slide has 4 bullet points:\n- Performance considerations\n- Subjective to the clustering algorithm and clustering parameters\n- Only certain attributes may have outlier properties, no need to disqualify the entire tuple\n- Contamination may occur by \"column\", not by row"}, {"start": "402.489", "end": "412.38", "text": "and look for outliers in these ones because you can put, you can basically look at the outliers and four outliers are very specific and fast wave.", "frame_description": " The image is a slide from a lecture video. The slide is titled \"Why a Special Technique to Identify Outliers?\". It has a blue background with white text.\n\nThe slide has the following text:\n\n\"Why not just modify clustering or other algorithms to detect outliers?\n\n- Performance considerations\n- Subjective to the clustering algorithm and clustering parameters\n- Only certain attributes may have outlier properties, no need to disqualify the entire tuple\n- Contamination may occur by \"column\", not by row\""}, {"start": "413.459", "end": "437.519", "text": "The contamination usually occurs by column by feature and not by road so much. So, and this again for the credit card, you look at, for example, the area where the, um, where, where, where the money is, is given away. So if you usually live in the US and all of a sudden you're in East Asia, it might have a reason. That's a normal reason like holiday, but it could also be fraud.", "frame_description": " The image is a slide from a lecture video. The slide is titled \"Why A Special Technique to Identify Outliers?\". It has a blue background with white text.\n\nThe slide has a bulleted list of three reasons why a special technique is needed to identify outliers. The reasons are:\n1. Performance considerations\n2. Subjective to the clustering algorithm and clustering parameters\n3. Contamination may occur by \"column\", not by row\n\nThere are no images or diagrams shown in the slide."}, {"start": "440.149", "end": "467.359", "text": "Let's take a look at the methods that we have for statistical based methods. I brought you two, the distribution based and the depth based one. And let's take a look at them. The distribution based outlet detection method is based on the assumptions that you do a discord test. So the assumption here is we have knowledge of the data, we know the statistical measures, we know the distribution, we know the mean, we know the variant", "frame_description": " ## Outlier Analysis\n- Introduction / Motivation / Definition\n- **Statistical-based Detection**\n - Distribution-based, depth-based\n- **Distance-based Detection**\n - Index-based, nested-loop, cell-based, local-outliers\n- Questions"}, {"start": "467.929", "end": "484.39", "text": "um that's a given and it's it's kind of realistic to know this even though as you see here, we have a normal distribution, a bell curve of the data set that's like the inherent model of, for example, in canines, right,", "frame_description": " The image is a slide from a lecture on statistical-based outlier detection. The slide is titled \"Statistical-Based Outlier Detection (Distribution-based)\".\n\nThe slide has the following notes:\n\n- Assumptions:\n - Knowledge of data (distribution, mean, variance)\n- Statistical discrepancy test\n - Data is assumed to be part of a working hypothesis (working hypothesis)\n - Each data object in the dataset is compared to the working hypothesis and is either accepted into the working hypothesis or rejected as discordant into an alternative hypothesis (outliers)\n\nThe slide also has the following diagram:\n\n- A normal distribution curve with the x-axis labeled \"IQ Test Scores\" and the y-axis labeled \"Number of People\". The curve is labeled \"Normal (Gaussian) Distribution\".\n- Two vertical lines are drawn on the curve, one at x=75 and one at x=125. The area under the curve between these two lines is shaded in light gray. The text \"Outliers\" is written in this shaded region.\n- The text \"Working Hypothesis: H0: o_i \\in F, where i=1,2,...,n\" is written below the distribution curve.\n- The text \"Discordancy Test: o_i is o in F within standard deviation = 15\" is written below the working hypothesis.\n- The text \"Alternative Hypothesis:\" is written below the discordancy test.\n- The text \"- Inherent Distribution: H1: o_i \\in G, where i=1,2,...,n\" is written below the alternative hypothesis.\n- The text \"- Mixture Distribution: H1: o_i = (1-z)*F + z*G, where i=1,2,...,n\" is written below the inherent distribution.\n- The text \"- Slippage Distribution: H1: o_i = e^{(z-1)*F} where i=1,2,...,n\" is written below the mixture distribution."}, {"start": "484.6", "end": "509.54", "text": "the outlier is found easily. So here we are based on models on we have a given statistical function the and we we from that statistical function, we do the the the outlier test. So for each of the data sets data points here including these two, we would do this discord test. The data is assumed to be part of a working hypothesis.", "frame_description": " The image is a slide from a lecture on statistical-based outlier detection. The slide is titled \"Statistical-Based Outlier Detection (Distribution-based)\".\n\nThe slide has the following notes:\n\n- Assumptions:\n  - Knowledge of data (distribution, mean, variance)\n- Statistical discrepancy test\n  - Data is assumed to be part of a working hypothesis (working hypothesis)\n  - Each data object in the dataset is compared to the working hypothesis and is either accepted into the working hypothesis or rejected as discordant into an alternative hypothesis (outliers)\n\nThe slide also has the following diagram:\n\n- A normal distribution curve with a superimposed IQ test scores distribution. The IQ test scores distribution is labeled with the mean and standard deviation. The diagram also shows the regions of the distribution that are considered to be outliers."}, {"start": "510.309", "end": "526.2", "text": "For example, that this function in the data is all part of this function. So we claim all data points are part of this discussion and function which wouldn't be correct for these two, right? And so is if", "frame_description": " The image is a slide from a lecture on statistical-based outlier detection. The slide is titled \"Statistical-Based Outlier Detection (Distribution-based)\".\n\nThe slide has the following notes:\n\nAssumptions:\n- Knowledge of data (distribution, mean, variance)\n\nStatistical discrepancy test\n- Data is assumed to be part of a working hypothesis (working hypothesis)\n- Each data object in the dataset is compared to the working hypothesis and is either accepted into the working hypothesis or rejected as discordant into an alternative hypothesis (outliers)\n\nThe slide also has the following diagram:\n\nThe diagram shows a normal distribution curve with the x-axis labeled \"IQ Test Scores\" and the y-axis labeled \"Frequency\". The curve is labeled \"Normal (Gaussian) Distribution\". There are two vertical lines on the graph labeled \"Outliers\". The area to the left of the first vertical line and the area to the right of the second vertical line are shaded in light gray. The area between the two vertical lines is not shaded.\n\nThe slide also has the following text:\n\nWorking Hypothesis: \\(H_0\\): \\(e \\sim F\\), where \\(i=1,2,...,n\\)\nDiscordancy Test: \\(is_0\\) in \\(F\\) within standard deviation = 15\nAlternative Hypothesis:\n- Inherent Distribution: \\(H_1\\): \\(o \\sim G\\), where \\(i=1,2,...,n\\)\n- Mixture Distribution: \\(H_0\\): \\(e=(1-z)F + zG\\), where \\(i=1,2,...,n\\)\n- Slippage Distribution: \\(H_0\\): \\(o \\sim (1-z)F\\), where \\(i=1,2,...,n\\)"}, {"start": "526.39", "end": "549.559", "text": "any data 0.0 I in this function is within a standard division of 15, which means it's a high variant. So higher weight, more weight than 15 points. Then we say OK, this is a discord see. So each data in the object of this is compared to this working hypothesis, which is this one here for the Gaussian function", "frame_description": " The image is a slide from a lecture on statistical-based outlier detection. The slide is titled \"Statistical-Based Outlier Detection (Distribution-based)\".\n\nThe slide has the following notes:\n\n- Assumptions:\n  - Knowledge of data (distribution, mean, variance)\n- Statistical discrepancy test\n- Data is assumed to be part of a working hypothesis (working hypothesis)\n- Each data object in the dataset is compared to the working hypothesis and is either accepted into the working hypothesis or rejected as discordant into an alternative hypothesis (outliers)\n\nThe slide also has the following diagram:\n\n- A normal distribution curve with the x-axis labeled \"IQ Test Scores\" and the y-axis labeled \"Frequency\". The curve is labeled \"Normal (Gaussian) Distribution\".\n- Two vertical lines are drawn on the curve, one at x=75 and one at x=125. The area under the curve between these two lines is shaded in light gray. The text \"Outliers\" is written in this shaded region.\n- A horizontal line is drawn at y=15. The text \"Working Hypothesis: H0: o_i \u2208 F, where i=1,2,...,n\" is written above this line.\n- A horizontal line is drawn at y=50. The text \"Discordancy Test: is o_i in F within standard deviation = 15\" is written below this line.\n- A horizontal line is drawn at y=100. The text \"Alternative Hypothesis: H1: o_e \u2208 G, where i=1,2,...,n\" is written above this line.\n- A horizontal line is drawn at y=250. The text \"- Mixture Distribution: H0: o_e = (1-Z)*F + Z*G, where i=1,2,...,n\" is written below this line.\n- A horizontal line is drawn at y=400. The text \"- Slippage Distribution: H0: o_e = (1-Z)*F + Z*F, where i=1,2,...,n\" is written below this line."}, {"start": "550.5", "end": "573.349", "text": "and is either accepted or rejected into an alternative hypothesis. The alternative hypothesis would be in this case, the data follows a different distribution like an inherent distribution, mixture distribution or slippage distribution. And these ones, whatever this one would be clearly are not part of the caution distribution.", "frame_description": " The image is a slide from a lecture on statistical-based outlier detection. The slide is titled \"Statistical-Based Outlier Detection (Distribution-based)\".\n\nThe slide has the following notes:\n\n- Assumptions:\n - Knowledge of data (distribution, mean, variance)\n- Statistical discrepancy test\n - Data is assumed to be part of a working hypothesis (working hypothesis)\n - Each data object in the dataset is compared to the working hypothesis and is either accepted into the working hypothesis or rejected as discordant into an alternative hypothesis (outliers)\n\nThe slide also has the following diagram:\n\n- A normal distribution curve with the X-axis labeled \"IQ Test Scores\" and the Y-axis labeled \"Frequency\". The curve is centered at 100 and has a standard deviation of 15. There are two vertical lines drawn at 75 and 125, which are labeled \"Outliers\"."}, {"start": "574.02", "end": "587.479", "text": "This is distribution based is typical for um model based clustering. So this would be the typical way of how to find an outlier when you know the model behind it", "frame_description": " The image is a slide from a lecture on statistical-based outlier detection. The slide is titled \"Statistical-Based Outlier Detection (Distribution-based)\".\n\nThe slide has the following notes:\n\n- Assumptions:\n - Knowledge of data (distribution, mean, variance)\n- Statistical discrepancy test\n - Data is assumed to be part of a working hypothesis (working hypothesis)\n - Each data object in the dataset is compared to the working hypothesis and is either accepted into the working hypothesis or rejected as discordant into an alternative hypothesis (outliers)\n\nThe slide also has the following diagram:\n\n- A scatter plot of IQ test scores. The x-axis is labeled \"IQ Test Scores\" and the y-axis is labeled \"Frequency\". The data points are normally distributed. There are two vertical lines drawn on the scatter plot, one at 75 and one at 125. The area between these two lines is labeled \"Working Hypothesis: H0: o \u2208 F, where i = 1,2,...,n\". The area outside of these two lines is labeled \"Alternative Hypothesis: H1: o \u2208 G, where i = 1,2,...,n\".\n- A box plot of the IQ test scores. The box plot shows the median, the first quartile, and the third quartile. The whiskers extend to the minimum and maximum values. There are two outliers shown in the box plot, one at 50 and one at 150. These outliers are also shown in the scatter plot."}, {"start": "589.419", "end": "611.039", "text": "distribution base is quite similar. The knowledge of the data is again a given and then you do the statistic of the student test. And, and basically this is this is the same thing, right as it was before. The, the the tricky part here is you have to know the Varian of this Gaussian function and", "frame_description": " The image is a slide from a lecture on statistical-based outlier detection. The slide is titled \"Statistical-Based Outlier Detection (Distribution-based)\".\n\nThe slide has the following notes:\n\nAssumptions:\n- Knowledge of data (distribution, mean, variance)\n\nStatistical discrepancy test\n- Data is assumed to be part of a working hypothesis (working hypothesis)\n- Each data object in the dataset is compared to the working hypothesis and is either accepted into the working hypothesis or rejected as discordant into an alternative hypothesis (outliers)\n\nThe slide also has the following diagram:\n\nThe diagram shows a normal distribution curve with the x-axis labeled \"IQ Test Scores\" and the y-axis labeled \"Frequency\". The curve is centered around 100 and has a standard deviation of 15. There are three vertical lines drawn on the curve, one at 75, one at 100, and one at 125. The line at 75 is labeled \"Lower Quartile\", the line at 100 is labeled \"Median\", and the line at 125 is labeled \"Upper Quartile\". There are also two horizontal lines drawn on the curve, one at y=500 and one at y=1500. The line at y=500 is labeled \"Lower Fence\" and the line at y=1500 is labeled \"Upper Fence\". Any data points that fall outside of the fences are considered to be outliers."}, {"start": "611.63", "end": "637.46", "text": "and you have to know the Gaussian function in and of itself. And I I must say as you know, this is an unsupervised learning, unsupervised training you usually do not know which function your data follows. This is part of the pattern matching. So it's not as simple as it is. But once you know the pattern, of course, it's easy to find that part, a few data points that are not following the pattern", "frame_description": " The image is a slide from a lecture on statistical-based outlier detection. The slide is titled \"Statistical-Based Outlier Detection (Distribution-based)\".\n\nThe slide has the following notes:\n\nAssumptions:\n- Knowledge of data (distribution, mean, variance)\n\nStatistical discordancy test\n- Data is assumed to be part of a working hypothesis (working hypothesis)\n- Each data object in the dataset is compared to the working hypothesis and is either accepted in the working hypothesis or rejected as discordant into an alternative hypothesis (outliers)\n\nThe slide also has the following diagram:\n\nThe diagram shows a normal distribution curve with the x-axis labeled \"IQ Test Scores\" and the y-axis labeled \"Frequency\". The curve is centered around 100 and has a standard deviation of 15. There are three vertical lines drawn on the curve at 75, 100, and 125. The line at 75 is labeled \"Outliers\". The area under the curve between 75 and 125 is shaded in light blue.\n\nThe diagram also has the following text:\n\n- Working Hypothesis: \\(H_0\\): \\(x_i \\in F\\), where \\(i=1,2,...,n\\)\n- Discordancy Test: \\(H_0\\) is in F within standard deviation = 15\n- Alternative Hypothesis:\n - Inherent Distribution: \\(H_1\\): \\(x_i \\in G\\), where \\(i=1,2,...,n\\)\n - Mixture Distribution: \\(H_0\\): \\(x_i \\sim (1-\\alpha)F + \\alpha G\\), where \\(i=1,2,...,n\\)\n - Slippage Distribution: \\(H_0\\): \\(x_i \\sim (1-\\alpha)F + \\alpha F_{\\alpha}\\), where \\(i=1,2,...,n\\)"}, {"start": "639.039", "end": "645.349", "text": "depth base. Here, the data is organized into layers according to some definition of depth.", "frame_description": " The image is a slide from a lecture on statistical-based outlier detection. The slide is titled \"Statistical-Based Outlier Detection (Depth-based)\".\n\nThe slide has the following notes:\n\n- Data is organized into layers according to some definition of depth.\n- Shallow layers are more likely to contain outliers than deep layers.\n- Can efficiently handle computation for k < 4.\n\nThe slide also has a diagram of a 3D scatter plot. The diagram shows a point cloud with three clusters. The clusters are labeled \"A\", \"B\", and \"C\". The points in cluster \"A\" are the outliers. The points in cluster \"B\" are the normal data points. The points in cluster \"C\" are the noisy data points."}, {"start": "645.869", "end": "673.44", "text": "So for example, this one would would be the the distance metric and then you have shallow layers that are more likely to contain out layers than deep layers. So here the deep layers are the ones. But for example, the density is very high. And this part would be a cluster, this part would be a cluster, this could be clusters, this could be a cluster, this little mountains, right and every everywhere where there is no depth, no height, right.", "frame_description": " ## Statistical-based outlier detection (depth-based)\n\n- Data is organized into layers according to some definition of depth.\n- Shallow layers are more likely to contain outliers than deep layers.\n- Can efficiently handle computation for k < 4.\n\nThe image shows a diagram of a statistical-based outlier detection method. The data is organized into a 3D grid, with each point representing a data point. The color of the point indicates the depth of the point, with darker points being closer to the surface and lighter points being deeper. The diagram shows that outliers are more likely to be found in the shallow layers of the grid, as these layers are more likely to contain noise and other anomalies."}, {"start": "673.77", "end": "679.109", "text": "This is a likelihood that it could contain the shallow layers could contain outlays,", "frame_description": " ## Statistical-Based Outlier Detection (Depth-based)\n\n- Data is organized into layers according to some definition of depth.\n- Shallow layers are more likely to contain outliers than deep layers.\n- Can efficiently handle computation for k < 4.\n\nThe image shows a diagram of a statistical-based outlier detection method. The data is organized into a 3D grid, with each point representing a data point. The color of the point indicates the depth of the point, with darker points being closer to the surface and lighter points being deeper. The diagram shows that outliers are more likely to be found in the shallow layers of the grid, as these layers are more likely to contain noise and other data irregularities."}, {"start": "679.739", "end": "697.799", "text": "it can efficiently handle the computation for pay smaller four. So this one is a quite slow one and as it cannot handle different parameterization very well. Um But this particular one that I picked up for you is actually a VL TB method", "frame_description": " ## Statistical-Based Outlier Detection (Depth-based)\n\n- Data is organized into layers according to some definition of depth.\n- Shallow layers are more likely to contain outliers than deep layers.\n- Can efficiently handle computation for k < 4.\n\nThe image shows a diagram of a 3D scatter plot. The x-axis is labeled \"Age\", the y-axis is labeled \"Income\", and the z-axis is labeled \"Education\". The data points are colored according to their depth, with shallow data points being colored red and deep data points being colored blue. The diagram shows that there are more outliers in the shallow layers of the data than in the deep layers."}, {"start": "697.95", "end": "712.229", "text": "which means in this case, it's it's a method created for very large data set and with large, it means in the rows but in this case also in the columns and that's that's a really strong one. It just has this limitation,", "frame_description": " ## Statistical-Based Outlier Detection (Depth-based)\n\n- Data is organized into layers according to some definition of depth.\n- Shallow layers are more likely to contain outliers than deep layers.\n- Can efficiently handle computation for k < 4.\n\nThe image shows a diagram of a statistical-based outlier detection method. The data is organized into layers, with the shallowest layer at the top and the deepest layer at the bottom. The color of each layer indicates the number of outliers in that layer. The diagram shows that the shallow layers have more outliers than the deeper layers. This is because the shallow layers are more likely to contain data that is different from the rest of the data. The diagram also shows that the method can efficiently handle computation for k < 4. This is because the method only needs to compute the depth of each data point, which is a constant time operation."}, {"start": "714.03", "end": "733.65", "text": "a statistical based outlay detection is the most um common form of outlay detection. Most outlay research has been done in this area, many data distributions can be found. And basically model based K means the clustering methods that are distribution based fit", "frame_description": " The image is a slide from a lecture on statistical-based outlier detection. The slide has the following text:\n\n## Statistical-Based Outlier Detection\n**Strengths**\n- Most outlier research has been done in this area, many data distributions are known.\n\n**Weakness**\n- Almost all of the statistical models are univariate (only handle one attribute) and those that are multivariate only efficiently handle k<4\n- All models assume the distribution is known - this is not always the case\n- Outlier detection is completely subjective to the distribution used"}, {"start": "734.159", "end": "745.82", "text": "basically one on one to this type of outlier detection. The weakness for this type of methods is that almost all of the statistical modes are univariate.", "frame_description": " The image is a slide from a lecture on statistical-based outlier detection. The slide is titled \"Statistical-Based Outlier Detection\".\n\nThe slide has three main sections: strengths, weaknesses, and notes.\n\nThe strengths section has one bullet point: \"Most outlier research has been done in this area, many data distributions are known.\"\n\nThe weaknesses section has four bullet points:\n\n- \"Almost all of the statistical models are univariate (only handle one attribute) and those that are multivariate only efficiently handle k<4\"\n- \"All models assume the distribution is known - this is not always the case\"\n- \"Outlier detection is completely subjective to the distribution used\"\n\nThe notes section is empty.\n\nThere are no images or diagrams shown in the slide."}, {"start": "746.08", "end": "775.119", "text": "It can only handle one attribute and those that are Multivariate only if can only efficiently handle K a smaller force. So here the K this was for the one in be before here, this one, the depth based one actually the number of features which in this case makes makes a significant negative impact, right? So we are, we, we cannot handle high data sets are high in the feature but only high in the lows.", "frame_description": " ## Statistical-Based Outlier Detection\n**Strengths:**\n- Most outlier research has been done in this area, many data distributions are known\n\n**Weakness:**\n- Almost all of the statistical models are univariate (only handle one attribute) and those that are multivariate only efficiently handle k<4\n- All models assume the distribution is known -> this is not always the case\n- Outlier detection is completely subjective to the distribution used"}, {"start": "776.479", "end": "790.919", "text": "All moms assume the distribution is known. This is not always the case. Basically, it's never the case, it's always an estimate of the distribution and the outline detection is completely subjective to distribution use.", "frame_description": " The image is a slide from a lecture on statistical-based outlier detection. The slide is titled \"Statistical-Based Outlier Detection\" and it lists the strengths and weaknesses of this approach.\n\nThe strengths listed are:\n- Most outlier research has been done in this area, many data distributions are known\n\nThe weaknesses listed are:\n- Almost all of the statistical models are univariate (only handle one attribute) and those that are multivariate only efficiently handle k<4\n- All models assume the distribution is known - this is not always the case\n- Outlier detection is completely subjective to the distribution used"}, {"start": "791.14", "end": "811.64", "text": "So if I would in in the case of the distribution based method, I assume it's not a Gaussian curve but a let's say logarithmic one, this logarithmic one would be something like this, right? Um Then it would be a very different result and I would have a very different amount of outlay.", "frame_description": " ## Statistical-Based Outlier Detection\n- Strengths\n  - Most outlier research has been done in this area, many data distributions are known\n- Weakness\n  - Almost all of the statistical models are univariate (only handle one attribute) and those that are multivariate only efficiently handle k<4\n  - All models assume the distribution is known -> this is not always the case\n  - Outlier detection is completely subjective to the distribution used"}, {"start": "815.75", "end": "841.9", "text": "The next type of um methods I want to show you is the distance based detection. Um particularly the local outliers ones. I like these ones the most because again, I was trained in it like in DB scan distance based detection belongs to the density based methods and clustering, at least they are of a similar type. We also have index based. This comes from database systems. Nested lobe is database system style and cell based", "frame_description": " The image is a slide from a lecture video on outlier analysis. The slide is titled \"Outlier Analysis - Outline\" and it lists the following topics:\n\n- Introduction / Motivation / Definition\n- Statistical-based Detection\n    - Distribution-based, depth-based\n- Distance-based Detection\n    - Index-based, nested-loop, cell-based, local-outliers\n- Questions\n\nThere are no additional notes or diagrams shown in the image."}, {"start": "842.179", "end": "855.03", "text": "which is um yeah, you could say it's a bit database but but not so much. Lots of you might know the nested blue groin um and the index, basic indexing structures and database systems", "frame_description": " The image is a slide from a lecture video on outlier analysis. The slide is titled \"Outlier Analysis - Outline\" and it lists the following topics:\n\n- Introduction / Motivation / Definition\n- Statistical-based Detection\n - Distribution-based, depth-based\n- Distance-based Detection\n - Index-based, nested-loop, cell-based, local-outliers\n- Questions\n\nThere are no additional notes or diagrams shown in the image."}, {"start": "856.75", "end": "878.075", "text": "um distance based outlay detection in general believe that an object O in a data set, for example. And the data set T is an outlier if at least a fraction of the objects are a certain distance away from an, an original data 0.0. And this 0.0", "frame_description": " The image is a slide from a lecture on distance-based outlier detection. The slide is titled \"Distance-Based Outlier Detection\".\n\nThe slide has the following text:\n\n\"Distance-based: An object O in a dataset T is a DB(p,d) outlier if at least fraction p of the objects in T are >= distance D from O.\n\nA point O in a dataset is an outlier with respect to parameters k and d if no more than k points in the dataset are at a distance of d or less from O.\n\nRelative measurement: Let D'(O) denote the distance of the k^th nearest neighbor of O. It is a measure of how much of an outlier point O is.\"\n\nThe slide also has the following diagram:\n\n[Image of a scatter plot with two clusters of points. One point is labeled \"O\" and is located far from the other points.]\n\nThe diagram shows a scatter plot with two clusters of points. One point, labeled \"O\", is located far from the other points."}, {"start": "878.085", "end": "899.419", "text": "in a data set is in an outlier with respect to the parameters that you have input parameters if not more than K points in the data set are at a distance of D or less from this 0.0. And this is basically you guys, it's the same as you remember in the clustering one. And this is your O", "frame_description": " The image is a slide from a lecture on distance-based outlier detection. The slide is titled \"Distance-Based Outlier Detection\".\n\nThe slide has the following text:\n\n\"Distance-based: An object O in a dataset T is a DB(p,D) outlier if at least fraction p of the objects in T are >= distance D from O.\n\nA point O in a dataset is an outlier with respect to parameters k and d if no more than k points in the dataset are at a distance of d or less from O.\n\nRelative measurement: Let D'(O) denote the distance of the k^th nearest neighbor of O. It is a measure of how much of an outlier point O is.\"\n\nThe slide also has a diagram showing a dataset with a number of points. The diagram shows the k^th nearest neighbor of a point O."}, {"start": "900.0", "end": "920.69", "text": "and then you have this epsilon environment, right? And then you have K other points in it, right? And these k other points are not in the same uh not, not too far away. So with respect to this parameter, they are not, they are not too far away. And that means", "frame_description": " The image is a slide from a lecture on distance-based outlier detection. The slide is titled \"Distance-Based Outlier Detection\".\n\nThe slide has the following text:\n\n\"Distance-based: An object O in a dataset T is a DB(p,d) outlier if at least fraction p of the objects in T are >= distance D from O.\n\nA point O in a dataset is an outlier with respect to parameters k and d if no more than k points in the dataset are at a distance of d or less from O.\n\nRelative measurement: Let D'(O) denote the distance of the k^th nearest neighbor of O. It is a measure of how much of an outlier point O is.\"\n\nThe slide also has a diagram of a dataset with a point O that is an outlier. The diagram shows the distance from O to its k^th nearest neighbor."}, {"start": "925.2", "end": "943.34", "text": "if so he the, the case should be a limiter, right? It cannot be too many of them, then it's an outlier. So it's the, it's, it uses a similar definition, but it is looking for the opposite, right? It looks for what's not inside or what's not close to this. O", "frame_description": " The image is about distance-based outlier detection.\n\nThe notes shown are:\n\n- Distance-based: An object O in a dataset T is a DB(p,d) outlier if at least fraction p of the objects in T are >= distance d from O.\n- A point O in a dataset is an outlier with respect to parameters k and d if no more than k points in the dataset are at a distance of d or less from O.\n- Relative measurement: Let D(O) denote the distance of the kth nearest neighbor of O. It is a measure of how much of an outlier point O is.\n\nThe image also shows a diagram of a circle with a point O inside it. The circle is labeled with the letter k. The diagram illustrates the concept of the kth nearest neighbor."}, {"start": "944.83", "end": "969.69", "text": "So here, if you have o on o is the point, you want to check for your outlier for. If it's an outlier, then you check, well, how many cases are close by? And if there are more than this amount that you allow to be close by, then it's not an outlier. But if it's less because there are not many points close by, it's an outlier. Now, this is not a cell.", "frame_description": " ## Distance-Based Outlier Detection\n- Distance-based: An object \\(O\\) in a dataset \\(T\\) is a DB(p,d) outlier if at least fraction \\(p\\) of the objects in \\(T\\) are >= distance \\(d\\) from \\(O\\).\n- A point \\(O\\) in a dataset is an outlier with respect to parameters \\(k\\) and \\(d\\) if no more than \\(k\\) points in the dataset are at a distance of \\(d\\) or less from \\(O\\).\n- Relative measurement: Let \\(D'(O)\\) denote the distance of the \\(k^{th}\\) nearest neighbor of \\(O\\). It is a measure of how much of an outlier point \\(O\\) is."}, {"start": "971.78", "end": "972.44", "text": "Bye bye.", "frame_description": " ## Distance-Based Outlier Detection\n- Distance-based: An object O in a dataset T is a DB(p,d) outlier if at least fraction p of the objects in T are >= distance d from O.\n- A point O in a dataset is an outlier with respect to parameters k and d if no more than k points in the dataset are at a distance of d or less from O.\n- Relative measurement: Let D'(O) denote the distance of the k^th nearest neighbor of O. It is a measure of how much of an outlier point O is.\n\nThe image shows a diagram of a circle with a point O inside it. The circle is labeled with the letter \"k\". The caption below the image says \"Relative measurement: Let D'(O) denote the distance of the k^th nearest neighbor of O. It is a measure of how much of an outlier point O is.\""}, {"start": "973.989", "end": "975.96", "text": "I'm going to dele this again.", "frame_description": " ## Distance-Based Outlier Detection\n- Distance-based: An object O in a dataset T is a DB(p,d) outlier if at least fraction p of the objects in T are >= distance d from O.\n- A point O in a dataset is an outlier with respect to parameters k and d if no more than k points in the dataset are at a distance of d or less from O.\n- Relative measurement: Let D(O) denote the distance of the kth nearest neighbor of O. It is a measure of how much of an outlier point O is."}, {"start": "978.219", "end": "988.489", "text": "This belongs to the neighborhood of things and it's, it should be quite clear for you guys when it's an O, it's basically the opposite of the clustering that we made.", "frame_description": " ## Distance-Based Outlier Detection\n- Distance-based: An object O in a dataset T is a DB(p,d) outlier if at least fraction p of the objects in T are >= distance d from O.\n- A point O in a dataset is an outlier with respect to parameters k and d if no more than k points in the dataset are at a distance of d or less from O.\n- Relative measurement: Let D(O) denote the distance of the kth nearest neighbor of O. It is a measure of how much of an outlier point O is."}, {"start": "989.859", "end": "997.059", "text": "Um If you lose indexes, then you could for example, use an artery or an art plus three", "frame_description": " The image is a slide from a lecture on distance-based outlier detection. The slide is titled \"Distance-Based Outlier Detection\".\n\nThe slide has the following text:\n\n\"Distance-based: An object O in a dataset T is a DB(p,d) outlier if at least fraction p of the objects in T are >= distance d from O.\n\nA point O in a dataset is an outlier with respect to parameters k and d if no more than k points in the dataset are at a distance of d or less from O.\n\nRelative measurement: Let D(O) denote the distance of the kth nearest neighbor of O. It is a measure of how much of an outlier point O is.\"\n\nThe slide also has the following diagram:\n\n[Image of a scatter plot with two clusters of points. One cluster has a single point that is far away from the other points.]\n\nThe diagram shows two clusters of points. One cluster has a single point that is far away from the other points. The point that is far away from the other points is an outlier."}, {"start": "997.57", "end": "1025.818", "text": "um that are built for multidimensional database systems. The index is basically there to speed up the computation and adds to the computation power to figure out how many data points are in a certain radius or in an epsilon environment around the object O that you want to test for. Is it an outlier or not the worst computation complexity. If you use a uh indexing structure,", "frame_description": " The image is a slide from a lecture on Index-based Algorithms. The slide is titled \"Index-based Algorithm [KN98]\".\n\nThe slide has the following notes:\n\n- Indexing Structures such as R-tree (R-tree+), K-D (K-D-B) tree are built for the multi-dimensional database\n- The index is used to search for neighbors of each object O within radius R around that object.\n- Once K (= N-1-p) neighbors of object O are found, O is not an outlier.\n- Worst-case computation complexity is O(K^n*log^2 n). K is the dimensionality and n is the number of objects in the dataset.\n- Pros: scale well with K\n- Cons: the index construction process may cost much time\n\nThe slide also has a diagram of an R-tree. The R-tree is a hierarchical data structure that can be used to index multi-dimensional data. The R-tree is shown as a tree with each node representing a region in the multi-dimensional space. The leaf nodes of the tree contain the data objects."}, {"start": "1028.18", "end": "1031.369", "text": "it would be OK times N squared", "frame_description": " The image is of a slide from a lecture on Index-based Algorithms. The slide is titled \"Index-based Algorithm [KN98]\".\n\nThe slide has the following notes:\n\n- Indexing Structures such as R-tree (R+-tree), K-D (K-D-B) tree are built for the multi-dimensional database\n- The index is used to search for neighbors of each object O within radius R around that object.\n- Once K (= N(1-p)) neighbors of object O are found, O is not an outlier.\n- Worst-case computation complexity is O(K^(n+2)). K is the dimensionality and n is the number of objects in the dataset.\n- Pros: scale well with K\n- Cons: the index construction process may cost much time\n\nThe slide also has a diagram of an R-tree. The R-tree is a hierarchical data structure that can be used to index multi-dimensional data. The R-tree is organized into a tree of nodes, where each node represents a region of the multi-dimensional space. The root node of the tree represents the entire space, and the child nodes represent successively smaller regions. The leaf nodes of the tree contain the data objects."}, {"start": "1032.64", "end": "1051.209", "text": "um And which is quite bad as you can imagine as a square uh is uh running slow. And so lay a detection as well as clustering can be quite slow. K here is the dimensionality of the feature space and N is the number of objects and number of flows.", "frame_description": " The image is a slide from a lecture on Index-based Algorithms. The slide is titled \"Index-based Algorithm [KN98]\".\n\nThe slide has the following notes:\n\n- Indexing Structures such as R-tree (R-tree+), K-D (K-D-B) tree are built for the multi-dimensional database\n- The index is used to search for neighbors of each object O within radius R around that object.\n- Once K (= N-1-p) neighbors of object O are found, O is not an outlier.\n- Worst-case computation complexity is O(K^n+2). K is the dimensionality and n is the number of objects in the dataset.\n- Pros: scale well with K\n- Cons: the index construction process may cost much time\n\nThe slide also has a diagram of an R-tree. The R-tree is a hierarchical data structure that can be used to index multi-dimensional data. The R-tree is shown as a tree with each node representing a region in the multi-dimensional space. The leaf nodes of the tree contain the data objects."}, {"start": "1051.51", "end": "1071.339", "text": "The pro of such an index based approach is it scales well with the number of the dimensions. So that's, that's the problem we had in the statistics based methods um that it didn't scale on all of the features and the dimensions, it would basically run in the curse of dimensionality", "frame_description": " The image is a slide from a lecture on Index-based Algorithms.\n\nThe slide is titled \"Index-based Algorithm [KN98]\".\n\nThe slide has the following text:\n\n- Indexing Structures such as R-tree (R-tree+), K-D (K-D-B) tree are built for the multi-dimensional database\n- The index is used to search for neighbors of each object O within radius R around that object.\n- Once K (= N(1-p)) neighbors of object O are found, O is not an outlier.\n- Worst-case computation complexity is O(K^n+2). K is the dimensionality and n is the number of objects in the dataset.\n- Pros: scale well with K\n- Cons: the index construction process may cost much time"}, {"start": "1071.68", "end": "1093.67", "text": "or not even work at all. Um The condo is that an indexing structure takes a lot of time to be built. It has, it has high processing time and the construction itself also needs time to be created. And, and basically that's the problem you have with database systems as well. Once you use an index, it's an expensive choice.", "frame_description": " The image is a slide from a lecture on Index-based Algorithms. The slide is titled \"Index-based Algorithm [KN98]\".\n\nThe slide has the following notes:\n\n- Indexing Structures such as R-tree (R-tree+), K-D (K-D-B) tree are built for the multi-dimensional database\n- The index is used to search for neighbors of each object O within radius R around that object.\n- Once K (= N(1-p)) neighbors of object O are found, O is not an outlier.\n- Worst-case computation complexity is O(K^n), K is the dimensionality and n is the number of objects in the dataset.\n- Pros: scale well with K\n- Cons: the index construction process may cost much time\n\nThe slide also has a diagram of an R-tree. The diagram shows how an R-tree is constructed for a set of 2-dimensional points. The R-tree is a hierarchical data structure that can be used to efficiently search for objects in a multi-dimensional space."}, {"start": "1097.219", "end": "1125.339", "text": "A nested loop algorithm for outlier detection very similar to the nested loop join. If you know it, it divides the buffer where you load the data into two halves, the first and the second areas, it breaks the data into blocks and then feeds the two blocks into these areas. You directly compute the distance between each pair of the objects of the two areas inside the area. And between the area, you decide who is an outlier or not depending on the distance.", "frame_description": ""}, {"start": "1125.56", "end": "1138.699", "text": "So the result of the distance, for example, Euclidean distance, it would tell you whether the higher, the more likely it's an outlier. You can basically also say, well, these are the candidates for being outliers and then you check them and refine them again,", "frame_description": ""}, {"start": "1139.25", "end": "1152.819", "text": "it has the same computational complexity as the indexing based algorithm. The pros are you avoid the index structure construction and you try to mini minimize the io the input output levels.", "frame_description": ""}, {"start": "1153.739", "end": "1157.27", "text": "It's a little bit of a better version than", "frame_description": " ## Nested-loop Algorithm (KN98)\n\n- Divides the buffer space into two halves (first and second arrays).\n- Break data into blocks and then feed two blocks into the arrays.\n- Directly computes the distance between each pair of objects inside the array or between arrays.\n- Here comes an example...\n- Same computational complexity as the index-based algorithm.\n- Pros: Avoid index structure construction.\n- Try to minimize the I/Os.\n\nAt the bottom of the image, there is a diagram showing how the algorithm works. The diagram shows how the data is divided into blocks and how the blocks are then processed by the algorithm."}, {"start": "1157.89", "end": "1185.849", "text": "um the indexing based one. So the example for the TED loop algorithm is here is the buffer, you have raarab, this is also the buffer buffer but now it's an area A and, and R AD and then you have the database or in this case, you can just call it the data. I don't want to call the data database. Uh Usually if you wouldn't use, like nowadays we use for data um data science.", "frame_description": " The image shows a slide from a lecture on Applied Data Science. The slide is titled \"Example - Stage 1\".\n\nThe slide has a diagram of a database with four rows and four columns. The first column is labeled \"Buffer\", the second column is labeled \"A\", the third column is labeled \"B\", and the fourth column is labeled \"D\". The first row is labeled \"Starting Point of Stage 1\" and the second row is labeled \"End Point of Stage 1\".\n\nThe diagram shows that the first step is to load block A from the database into the buffer. The second step is to load block B from the database into the first array. The third step is to load block C from the database into the second array. The fourth step is to load block D from the database into the second array.\n\nThe slide also states that the total number of reads is 4."}, {"start": "1186.06", "end": "1214.41", "text": "Um UDFS use a defined function in Python and Jupiter notebooks, it's also quite common to load a database system. And once you have a database system, it's much easier to connect with indexing structures and allow for methods that are optimized by the optimizer inside the database system itself. So it might sometimes be very helpful to look into the database system that you're using and check if they have an inbuilt machine learning approach.", "frame_description": " The image shows a slide from a lecture on Applied Data Science. The slide is titled \"Example - Stage 1\".\n\nThe slide has a diagram of a database with four rows and four columns. The columns are labeled A, B, C, and D. The rows are labeled \"Buffer\", \"Starting Point of Stage 1\", and \"End Point of Stage 1\".\n\nThe \"Buffer\" row has a blue block in the A column and a blue block in the B column.\n\nThe \"Starting Point of Stage 1\" row has a blue block in the A column and a blue block in the D column.\n\nThe \"End Point of Stage 1\" row has a blue block in the A column and a blue block in the C column.\n\nThe slide also has some notes. The notes say:\n\n\"A is the target block on stage 1\n\nLoad A into the first array (1R)\n\nLoad B into the second array (1R)\n\nLoad C into the second array (1R)\n\nLoad D into the second array (1R)\n\nTotal: 4 Reads\""}, {"start": "1214.67", "end": "1233.56", "text": "Um Something like this uh nested loop algorithm is likely um inbuilt into any database system. I'm not sure if Python can run it then, but I'm pretty sure it's possible to run it immediately if you look at the API for the database system.", "frame_description": " The image shows a slide from a lecture on Applied Data Science. The slide is titled \"Example - Stage 1\".\n\nThe slide has a diagram of a database with four rows and four columns. The columns are labeled A, B, C, and D. The rows are labeled \"Buffer\", \"Starting Point of Stage 1\", and \"End Point of Stage 1\".\n\nThe \"Buffer\" row has the value \"A\" in the first column and the value \"B\" in the second column.\n\nThe \"Starting Point of Stage 1\" row has the value \"A\" in the first column and the value \"D\" in the fourth column.\n\nThe \"End Point of Stage 1\" row has the value \"A\" in the first column and the value \"B\" in the second column.\n\nThe slide also has some notes. The notes say:\n\n\"A is the target block on stage 1\n\nLoad A into the first array (1R)\n\nLoad B into the second array (1R)\n\nLoad C into the second array (1R)\n\nLoad D into the second array (1R)\n\nTotal: 4 Reads\""}, {"start": "1234.76", "end": "1244.28", "text": "But anyway, it's a database or, or data here just calls for the data in the databases and not for how the database does it in detail.", "frame_description": " The image shows an example of stage 1 of a database buffer. The buffer has two arrays, each with a capacity of two blocks. The database has four blocks, labeled A, B, C, and D. The starting point of stage 1 is with block A in the buffer and blocks B, C, and D in the database. The first step is to load block A into the first array, which requires one read. The next step is to load block B into the second array, which requires another read. The third step is to load block C into the second array, which requires a third read. The final step is to load block D into the second array, which requires a fourth read. At the end of stage 1, all four blocks are in the buffer and the total number of reads is four."}, {"start": "1245.119", "end": "1274.319", "text": "Um But let's assume that the data here is in the buffer, this is in the, in the storage and the normal data. And here you have four arrays and there are two. And this is basically the buffer is how you swap it in main memory. And this is basically how it, because it's an optimization technique for finding outliers, you, you manage to load all these areas one by one into the buffer and check it with the first area", "frame_description": " The image shows a slide from a lecture on Applied Data Science. The slide is titled \"Example - Stage 1\".\n\nThe slide has a diagram with two columns. The left column is labeled \"Buffer\" and the right column is labeled \"DB\". The buffer column has four rows and the DB column has two rows.\n\nThe first row in the buffer column has a blue block labeled \"A\". The second row in the buffer column has a blue block labeled \"B\". The third row in the buffer column has a blue block labeled \"C\". The fourth row in the buffer column has a blue block labeled \"D\".\n\nThe first row in the DB column has a blue block labeled \"A\". The second row in the DB column has a blue block labeled \"D\".\n\nThe slide also has some text. The text says:\n\n\"A is the target block on stage 1\n\nLoad A into the first array (1R)\n\nLoad B into the second array (1R)\n\nLoad C into the second array (1R)\n\nLoad D into the second array (1R)\n\nTotal: 4 Reads\"\n\nThe text also says:\n\n\"Starting Point of Stage 1\"\n\nand\n\n\"End Point of Stage 1\""}, {"start": "1274.63", "end": "1302.589", "text": "in total. You have four reads and four comparisons. That's stage one. And in stage two, what you basically do, you now fix the last area, the D before you fix the A and then you check here, you iterate all the ones ABC without the D and then you have two reads because you don't need to read A anymore. A is done on ad in section from the beginning", "frame_description": " The image shows a slide from a lecture on Applied Data Science. The slide is titled \"Example - Stage 1\".\n\nThe slide has a diagram of a database with four blocks, labeled A, B, C, and D. The diagram shows that block A is the target block on stage 1. The slide also has a table that shows the starting point and end point of stage 1. The starting point is block A, and the end point is block D. The slide also has a list of the steps that are performed in stage 1. The steps are:\n1. Load A into the first array (1R)\n2. Load B into the second array (1R)\n3. Load C into the second array (1R)\n4. Load D into the second array (1R)\n\nThe slide also has a note that the total number of reads in stage 1 is 4."}, {"start": "1303.77", "end": "1312.63", "text": "in stage three, you do the same totally again to reads and two reads again. So this is basically how it optimizes", "frame_description": " The image shows an example of stage 3 of a database buffer. The buffer is a temporary storage area in memory that is used to speed up access to data. The database is a collection of data that is stored on a disk.\n\nThe image shows a table with three columns. The first column is labeled \"Buffer\", the second column is labeled \"DB\", and the third column is labeled \"Notes\".\n\nThe \"Buffer\" column shows the contents of the buffer at different points in time. The \"DB\" column shows the contents of the database. The \"Notes\" column provides additional information about the operations that are being performed.\n\nThe first row of the table shows the initial state of the system. The buffer is empty and the database contains the data blocks A, B, C, and D.\n\nThe second row of the table shows the state of the system after the C block has been loaded into the buffer. The C block is the target block for stage 3.\n\nThe third row of the table shows the state of the system after the D block has been loaded into the buffer. The D block is loaded into the buffer because it is needed to replace the C block.\n\nThe fourth row of the table shows the state of the system after the A block has been loaded into the buffer. The A block is loaded into the buffer because it is needed to replace the D block.\n\nThe fifth row of the table shows the state of the system after the B block has been loaded into the buffer. The B block is loaded into the buffer because it is needed to replace the A block.\n\nThe sixth row of the table shows the final state of the system. The buffer contains the data blocks A, B, and C. The database contains the data blocks D.\n\nThe image also shows a diagram of the buffer and the database. The diagram shows how the data blocks are stored in the buffer and the database."}, {"start": "1313.959", "end": "1338.15", "text": "um last but not least we have the local outliers. This was uh created, you see the same here. So some outliers can be defined as global and some can be defined as local outliers. And this was, was semantically quite a big step in research to, to distinguish these two automatically. Again, here you have the global outlier 01 and here, the local outlier 02", "frame_description": " The image is a slide from a lecture on distance-based outlier detection. The slide is titled \"Distance-Based Outlier Detection (Local Outliers)\".\n\nThe slide has the following notes:\n\n- Some outliers can be defined as global outliers, some can be defined as local outliers to a given cluster.\n- O2 would not normally be considered an outlier with regular distance-based outlier detection, since it looks at the global picture.\n\nThe slide also has a diagram showing two clusters of points. The diagram is labeled \"C1\" and \"C2\". The point labeled \"O1\" is a local outlier to the cluster C1. The point labeled \"O2\" is not an outlier."}, {"start": "1338.369", "end": "1362.569", "text": "um 02 would not normally be considered an outlier with regular distance based outlay detection. Also not necessarily with distribution based outlay detection. Um As all of these look at the global picture. So love or local outliers were the first time able to find these uh or two outliers here you see.", "frame_description": " The image is a slide from a lecture on distance-based outlier detection. The slide is titled \"Distance-Based Outlier Detection (Local Outliers)\".\n\nThe slide has the following notes:\n\n- Some outliers can be defined as global outliers, some can be defined as local outliers to a given cluster\n- O2 would not normally be considered an outlier with regular distance-based outlier detection, since it looks at the global picture\n\nThe slide also has a diagram showing two clusters of points. The first cluster is labeled \"C1\" and the second cluster is labeled \"C2\". The point labeled \"O1\" is a global outlier, as it is far from both clusters. The point labeled \"O2\" is a local outlier to the cluster C1, as it is far from the other points in the cluster."}, {"start": "1363.099", "end": "1378.64", "text": "And the idea very similar to DB scan, it comes from from the same um research, each data object is assigned a local outlier factor. So these are the ones that are factored. You have one point P that you check, you have", "frame_description": " ## Distance-Based Outlier Detection (Local Outliers)\n\n- Each data object is assigned a local outlier factor (LOF).\n- Objects which are closer to dense clusters receive a higher LOF.\n- LOF varies according to the parameter MinPts.\n\nThe image shows a diagram of how LOF is calculated for a data point. The data point is represented by a circle, and the other data points are represented by dots. The MinPts parameter is set to 3, which means that the LOF of the data point will be calculated using the three nearest data points. The distance between the data point and each of the three nearest data points is calculated, and the maximum of these distances is used to calculate the LOF. In this case, the maximum distance is d_max, and the LOF is calculated as follows:\n\nLOF = (d_max / d_min) / (number of neighbors)\n\nwhere d_min is the distance to the nearest neighbor."}, {"start": "1378.8", "end": "1402.13", "text": "this epsilon environment or you call it D MD max min points similar to DB scan here. This parameter uh defines how many are maximally allowed in this area for this thing to still be a uh an outlier. And then objects which are closer to dense clues receive a higher local outlier factor", "frame_description": " ## Local Outlier Factor (LOF)\n- Each data object is assigned a local outlier factor (LOF).\n- Objects which are closer to dense clusters receive a higher LOF.\n- LOF varies according to the parameter MinPts.\n\nThe image shows a diagram of how LOF is calculated. \n- The blue circle represents a data object. \n- The red circle represents the neighborhood of the data object, where k is the number of neighbors. \n- The green circle represents the distance from the data object to its kth neighbor. \n- The LOF of the data object is calculated as the ratio of the average distance from the data object to its k nearest neighbors."}, {"start": "1402.14", "end": "1427.53", "text": "than the ones who are farther away than very dense clusters of the density of this area, which is the density connectedness as you remember plays a crucial role. And the love varies according to the parameter min points, MIN PTS because this is basically how close it actually is in comparison to these others. So local outliers", "frame_description": " ## Distance-Based Outlier Detection (Local Outliers)\n\n- Each data object is assigned a local outlier factor (LOF).\n- Objects which are closer to dense clusters receive a higher LOF.\n- LOF varies according to the parameter MinPts.\n\nThe image shows a diagram of how the LOF is calculated for a data point. The data point is represented by a blue circle, and the other data points are represented by gray circles. The MinPts parameter is set to 3, which means that the LOF will be calculated using the three nearest neighbors of the data point. The distance from the data point to each of its neighbors is shown by a line, and the maximum distance is shown by a red line. The LOF is calculated by dividing the average distance from the data point to its neighbors by the maximum distance. In this case, the LOF is 2.5."}, {"start": "1427.8", "end": "1432.39", "text": "uh in in in pictures, if you take a look at this,", "frame_description": " The image is a slide from a lecture on distance-based outlier detection. The slide is titled \"Distance-Based Outlier Detection (Local Outliers)\".\n\nThe slide has two main parts. The left part of the slide has a diagram showing a number of data points in two dimensions. The data points are shown as black dots. There are five clusters of data points. Four of the clusters are well-separated from each other. The fifth cluster is overlapping with the other clusters.\n\nThe right part of the slide has a bar chart showing the outlier factor for each data point. The outlier factor is a measure of how different a data point is from its neighbors. The data points with the highest outlier factors are the most likely to be outliers.\n\nThe slide also has some notes. The notes say:\n\n- Distance-based outlier detection is a method for finding outliers based on their distance from their neighbors.\n- The outlier factor is a measure of how different a data point is from its neighbors.\n- Data points with high outlier factors are more likely to be outliers."}, {"start": "1432.939", "end": "1447.859", "text": "if you plot the local outlier factor, right in 3d, these ones, these data points that are actual clusters year round and very clearly defined. And then the outliers, if it's for sure an outlet, the more global it is", "frame_description": " The image is a slide from a lecture on distance-based outlier detection. The slide is titled \"Distance-Based Outlier Detection (Local Outliers)\".\n\nThe slide has a diagram of a 3D scatter plot. The diagram shows a number of data points in 3D space. The data points are colored blue and black. The blue data points represent the normal data points. The black data points represent the outliers.\n\nThe slide also has a number of notes written on it. The notes are as follows:\n\n* Distance-based outlier detection is a method for detecting outliers based on their distance from other data points.\n* Local outliers are data points that are significantly different from their neighbors.\n* Distance-based outlier detection methods can be used to detect both global and local outliers.\n* Global outliers are data points that are significantly different from all other data points in the dataset.\n* Local outliers are data points that are significantly different from their neighbors.\n* Distance-based outlier detection methods are simple to implement and can be used on large datasets.\n* Distance-based outlier detection methods are not robust to noise and can be sensitive to the choice of distance metric."}, {"start": "1448.469", "end": "1450.28", "text": "the more higher", "frame_description": " ## Distance-Based Outlier Detection (Local Outliers)\n\n- Outlier factor (OF): A measure of how much of an outlier a point is. \n- Local OF: OF based on distance to k nearest neighbors.\n- Global OF: OF based on distance to all other points.\n- k-NN distance: the distance to the kth nearest neighbor.\n- A point is an outlier if its OF is significantly higher than the OFs of its neighbors.\n\nThe image shows a 3D scatter plot of a dataset with four clusters. The points in the cluster on the bottom right have a higher OF than the points in the other clusters. This is because the points in the bottom right cluster are further away from their neighbors than the points in the other clusters."}, {"start": "1450.959", "end": "1474.17", "text": "is the last local outlier factor. So this is just the outlier factor. So the global outlier factor is bigger. But if you would put the local outlier factor, usually something that is closer to these very dense cluster like this one would be very, very large. I think this is this one because this is global and it is a global outlier. This one is huge factor.", "frame_description": " ## Distance-Based Outlier Detection (Local Outliers)\n\n- Outlier factor (OF): A measure of how much of an outlier a point is. \n- Local Outlier Factor (LOF): A measure of how much of an outlier a point is, compared to its neighbors.\n- LOF is calculated by comparing the average distance of a point to its k nearest neighbors, to the average distance of the k nearest neighbors of the k nearest neighbors.\n- A point with a high LOF is considered to be an outlier.\n\nThe image shows a scatter plot of data points, with the LOF of each point represented by a color. The points with the highest LOF are shown in red, and the points with the lowest LOF are shown in blue. The image also shows the k nearest neighbors of each point, which are used to calculate the LOF."}, {"start": "1476.29", "end": "1477.109", "text": "Mhm", "frame_description": " The image is a slide from a lecture on distance-based outlier detection. The slide is titled \"Distance-Based Outlier Detection (Partition-based)\".\n\nThe slide has the following notes:\n\n- Partition points using clustering algorithm\n- Compute upper and lower bounds for partitions\n- Identify candidate partitions containing outliers\n- Compute n outliers from points in candidate partitions\n\n\nThe slide also has the following diagram:\n\n[Image of a flowchart for distance-based outlier detection]\n\nThe flowchart has the following steps:\n\n1. Partition the data points into clusters.\n2. Compute the upper and lower bounds for each cluster.\n3. Identify the candidate clusters that contain outliers.\n4. Compute the n outliers from the points in the candidate clusters."}, {"start": "1478.04", "end": "1501.989", "text": "OK. I have one more for you guys, the partition based one partition based clustering method is called birch. It uses a partition of points that uses a clustering algorithm. You compute the upper and lower bounds with these partitions and you identify candidate partitions and containing outliers and then you compute the n outliers from the points and candidate partitions.", "frame_description": " The image is a slide from a lecture on distance-based outlier detection. The slide is titled \"Distance-Based Outlier Detection (Partition-based)\".\n\nThe slide has the following notes:\n\n* Partition points using clustering algorithm\n* Compute upper and lower bounds for partitions\n* Identify candidate partitions containing outliers\n* Compute n outliers from points in candidate partitions\n\n\nThe slide also has the following diagram:\n\n[Image of a flowchart for distance-based outlier detection]\n\nThe flowchart has the following steps:\n\n1. Partition the data points into clusters using a clustering algorithm.\n2. Compute the upper and lower bounds for each cluster.\n3. Identify the candidate partitions that contain outliers.\n4. Compute the n outliers from the points in the candidate partitions."}, {"start": "1502.5", "end": "1520.01", "text": "So first we build your cluster which is a clustering approach, your cluster in partitions of non outliers, you prune these partitions that they do not contain outliers. And then you use these indexing or nested loop algorithms that we discussed before on the remaining data points.", "frame_description": " The image is a slide from a lecture on distance-based outlier detection. The slide is titled \"Distance-Based Outlier Detection (Partition-based)\".\n\nThe slide has the following notes:\n\n* Partition points using clustering algorithm\n* Compute upper and lower bounds for partitions\n* Identify candidate partitions containing outliers\n* Compute n outliers from points in candidate partitions\n\nThe slide also has the following diagram:\n\n[Image of a diagram showing the steps of distance-based outlier detection.]\n\nThe diagram shows the steps of distance-based outlier detection. The first step is to partition the data points into clusters. The second step is to compute the upper and lower bounds for each partition. The third step is to identify the candidate partitions that contain outliers. The fourth step is to compute the n outliers from the points in the candidate partitions."}, {"start": "1520.29", "end": "1541.18", "text": "Um As many data points are removed during pruning the efficiency of indexing and naive lag significantly improved. So this version, the partition based one using B which is first clustering, then outlay detection helps here a lot in terms of uh computation power.", "frame_description": " The image is a slide from a lecture on distance-based outlier detection. The slide is titled \"Distance-Based Outlier Detection (Partition-based)\".\n\nThe slide has the following notes:\n\n- Partition points using clustering algorithm\n- Compute upper and lower bounds for partitions\n- Identify candidate partitions containing outliers\n- Compute n outliers from points in candidate partitions\n\n\nThe slide also has the following diagram:\n\n[Image of a flowchart for distance-based outlier detection]\n\nThe flowchart has the following steps:\n\n1. Partition the data points into clusters using a clustering algorithm.\n2. Compute the upper and lower bounds for each partition.\n3. Identify the candidate partitions that contain outliers.\n4. Compute the n outliers from the points in the candidate partitions."}, {"start": "1543.28", "end": "1555.16", "text": "If you have questions as this is an online um situation, please write me an email and um I'm seeing you next week. Thank you. Take care.", "frame_description": " The image is a slide from a lecture video. It has a dark background with light colored text. The text says \"Questions?\". In the top left corner of the image is the course information \"CSPC 4300/6300: Applied Data Science\". In the top right corner is the university logo with the text \"Engineering, Computing, and Applied Sciences\". There is a blue bar on the left side of the image."}]}