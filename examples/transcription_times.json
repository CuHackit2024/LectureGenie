{"2.309-14.1": "So, hello everyone and welcome to applied data science today, we're going to do the next big unsupervised learning technique which is called outlier detection.", "14.609-30.51": "Outlay detection is very closely connected to clustering as it is basically the opposite of finding clusters. If a data point is in a cluster, that's not the outlier. And if it's not in any cluster, it's an outlier.", "31.479-36.74": "So what's outline detection and why do we need it", "37.29-60.0": "outlier analysis is one of the most famous and most important analysis. We can do think about your credit card if you go to a different country that your credit card checks. If it's a fraud or not, this is called fraud detection. And it's used for credit cards, for telecommunications, any form of criminal activity in, for example, ecommerce", "60.74-73.839": "lay detection is also very much used in customized marketing, high and low income buying habits. What is it that you usually buy, what can be done to give you more to buy", "74.739-99.309": "medical treatments might be very interesting too. So unusual responses to various drugs, typical outlier detection task and the analysis of performance statistics. For example, in professional athletes, some people behave in an outlier ish way, but also it could be particularly good or particularly bad. That would be an outlier of your standard performance", "100.379-129.309": "weather prediction. Everything that is in extreme weather like a hurricane, a tornado, an earthquake, these are outliers, all of them because the most normal weather doesn't have them. So if you look at 40 years of weather, the tornado will be a rare occurrence and with that, it's an outlier. So all weather prediction for such extreme nature catastrophes would be an outlier detection.", "130.839-148.08": "Um financial applications like loan approvals and stock trading also strongly rely on outlier analysis. So as we say in statistics, one person's noise could be another person's signal. This is the motivation for outlier analysis.", "149.1-166.13": "What is an outlier and out any outlier is by definition an observation inconsistent with the rest of the data set that's also called a global outlier. A special outlier. For example, a local outlier is an observation inconsistent with their neighborhood,", "166.309-191.725": "a local instability or discontinuity. So if we look, for example, at um cluster two, this small dense cluster, very dark and cluster C one is more variant. It's a very loosely connected cluster, that's the global data set. And from both these clusters, the global outlier is 01, you see that little point here and the", "191.735-217.16": "local outlier is 02 because it's an outlier in comparison to the density of C two but not as a global one. If you look at the density existing in C one in comparison to C one, this would be the normal distance from its other data points. Nevertheless, because C two is so dense, this 02 is a local outlier", "219.0-242.72": "causes of outliers could be poor data quality or some contamination, low quality measurements, more functioning equipment or manual error. Besides all these kind of error situations, most data sets hold out layers. So this are correct data but exceptions to the existing pattern in the data.", "242.899-250.94": "While clustering looks at the patterns, the outlet detection looks at the exceptions. Both of them are necessary and important.", "252.869-280.75": "The object objective and outlay detection approaches is to define what data can be considered as inconsistent in a given data set for that. This is why I'm saying they strongly connect on the unsupervised level. You need to know what is consistent in a given data set as we are often more interested in patent motives. In this case, we look for the inconsistency that is not necessarily noisy.", "281.47-296.299": "Um We decide we distinguish two different types of outliers, statistic based outlier detection and distance based outline detection. I omitted the one that is in database systems. Um But that would be a third option.", "296.959-312.489": "We will find an efficient method to mine these outliers depending on how many features you have. If you have a high dimensionality, a lot and a high amount of variables mining these outliers can be very tricky.", "315.369-335.26": "Um One, some of, you might ask, well, if we have the pattern, if we know the pattern through clustering, for example, or through some functional algorithm, some function generator, why do we even need a special technique to only identify the outliers?", "335.929-357.869": "So what we could instead do, we could find the pattern and then everything that's not in the pattern is the outlier. And of course, these type of methods exist. But there are reasons why you may want to choose a technique that only identifies outliers. And one of the foremost considerations here are performance issues.", "358.39-377.239": "If you have to understand all the existing patterns in a data set first and name them efficiently, it might be faster to immediately look at the outlier. It's also another reason to subjective to the clustering algorithm, the clustering parameters if they run really slow.", "377.779-402.049": "Um Also we have sometimes only specific attributes that may have interesting outliers, but there is no need to disqualify the entire table. So if I'm looking at the credit card, for example, it's not so important how much money is changed, but it might be important where it is changed. So you look at specific features", "402.489-412.38": "and look for outliers in these ones because you can put, you can basically look at the outliers and four outliers are very specific and fast wave.", "413.459-437.519": "The contamination usually occurs by column by feature and not by road so much. So, and this again for the credit card, you look at, for example, the area where the, um, where, where, where the money is, is given away. So if you usually live in the US and all of a sudden you're in East Asia, it might have a reason. That's a normal reason like holiday, but it could also be fraud.", "440.149-467.359": "Let's take a look at the methods that we have for statistical based methods. I brought you two, the distribution based and the depth based one. And let's take a look at them. The distribution based outlet detection method is based on the assumptions that you do a discord test. So the assumption here is we have knowledge of the data, we know the statistical measures, we know the distribution, we know the mean, we know the variant", "467.929-484.39": "um that's a given and it's it's kind of realistic to know this even though as you see here, we have a normal distribution, a bell curve of the data set that's like the inherent model of, for example, in canines, right,", "484.6-509.54": "the outlier is found easily. So here we are based on models on we have a given statistical function the and we we from that statistical function, we do the the the outlier test. So for each of the data sets data points here including these two, we would do this discord test. The data is assumed to be part of a working hypothesis.", "510.309-526.2": "For example, that this function in the data is all part of this function. So we claim all data points are part of this discussion and function which wouldn't be correct for these two, right? And so is if", "526.39-549.559": "any data 0.0 I in this function is within a standard division of 15, which means it's a high variant. So higher weight, more weight than 15 points. Then we say OK, this is a discord see. So each data in the object of this is compared to this working hypothesis, which is this one here for the Gaussian function", "550.5-573.349": "and is either accepted or rejected into an alternative hypothesis. The alternative hypothesis would be in this case, the data follows a different distribution like an inherent distribution, mixture distribution or slippage distribution. And these ones, whatever this one would be clearly are not part of the caution distribution.", "574.02-587.479": "This is distribution based is typical for um model based clustering. So this would be the typical way of how to find an outlier when you know the model behind it", "589.419-611.039": "distribution base is quite similar. The knowledge of the data is again a given and then you do the statistic of the student test. And, and basically this is this is the same thing, right as it was before. The, the the tricky part here is you have to know the Varian of this Gaussian function and", "611.63-637.46": "and you have to know the Gaussian function in and of itself. And I I must say as you know, this is an unsupervised learning, unsupervised training you usually do not know which function your data follows. This is part of the pattern matching. So it's not as simple as it is. But once you know the pattern, of course, it's easy to find that part, a few data points that are not following the pattern", "639.039-645.349": "depth base. Here, the data is organized into layers according to some definition of depth.", "645.869-673.44": "So for example, this one would would be the the distance metric and then you have shallow layers that are more likely to contain out layers than deep layers. So here the deep layers are the ones. But for example, the density is very high. And this part would be a cluster, this part would be a cluster, this could be clusters, this could be a cluster, this little mountains, right and every everywhere where there is no depth, no height, right.", "673.77-679.109": "This is a likelihood that it could contain the shallow layers could contain outlays,", "679.739-697.799": "it can efficiently handle the computation for pay smaller four. So this one is a quite slow one and as it cannot handle different parameterization very well. Um But this particular one that I picked up for you is actually a VL TB method", "697.95-712.229": "which means in this case, it's it's a method created for very large data set and with large, it means in the rows but in this case also in the columns and that's that's a really strong one. It just has this limitation,", "714.03-733.65": "a statistical based outlay detection is the most um common form of outlay detection. Most outlay research has been done in this area, many data distributions can be found. And basically model based K means the clustering methods that are distribution based fit", "734.159-745.82": "basically one on one to this type of outlier detection. The weakness for this type of methods is that almost all of the statistical modes are univariate.", "746.08-775.119": "It can only handle one attribute and those that are Multivariate only if can only efficiently handle K a smaller force. So here the K this was for the one in be before here, this one, the depth based one actually the number of features which in this case makes makes a significant negative impact, right? So we are, we, we cannot handle high data sets are high in the feature but only high in the lows.", "776.479-790.919": "All moms assume the distribution is known. This is not always the case. Basically, it's never the case, it's always an estimate of the distribution and the outline detection is completely subjective to distribution use.", "791.14-811.64": "So if I would in in the case of the distribution based method, I assume it's not a Gaussian curve but a let's say logarithmic one, this logarithmic one would be something like this, right? Um Then it would be a very different result and I would have a very different amount of outlay.", "815.75-841.9": "The next type of um methods I want to show you is the distance based detection. Um particularly the local outliers ones. I like these ones the most because again, I was trained in it like in DB scan distance based detection belongs to the density based methods and clustering, at least they are of a similar type. We also have index based. This comes from database systems. Nested lobe is database system style and cell based", "842.179-855.03": "which is um yeah, you could say it's a bit database but but not so much. Lots of you might know the nested blue groin um and the index, basic indexing structures and database systems", "856.75-878.075": "um distance based outlay detection in general believe that an object O in a data set, for example. And the data set T is an outlier if at least a fraction of the objects are a certain distance away from an, an original data 0.0. And this 0.0", "878.085-899.419": "in a data set is in an outlier with respect to the parameters that you have input parameters if not more than K points in the data set are at a distance of D or less from this 0.0. And this is basically you guys, it's the same as you remember in the clustering one. And this is your O", "900.0-920.69": "and then you have this epsilon environment, right? And then you have K other points in it, right? And these k other points are not in the same uh not, not too far away. So with respect to this parameter, they are not, they are not too far away. And that means", "925.2-943.34": "if so he the, the case should be a limiter, right? It cannot be too many of them, then it's an outlier. So it's the, it's, it uses a similar definition, but it is looking for the opposite, right? It looks for what's not inside or what's not close to this. O", "944.83-969.69": "So here, if you have o on o is the point, you want to check for your outlier for. If it's an outlier, then you check, well, how many cases are close by? And if there are more than this amount that you allow to be close by, then it's not an outlier. But if it's less because there are not many points close by, it's an outlier. Now, this is not a cell.", "971.78-972.44": "Bye bye.", "973.989-975.96": "I'm going to dele this again.", "978.219-988.489": "This belongs to the neighborhood of things and it's, it should be quite clear for you guys when it's an O, it's basically the opposite of the clustering that we made.", "989.859-997.059": "Um If you lose indexes, then you could for example, use an artery or an art plus three", "997.57-1025.818": "um that are built for multidimensional database systems. The index is basically there to speed up the computation and adds to the computation power to figure out how many data points are in a certain radius or in an epsilon environment around the object O that you want to test for. Is it an outlier or not the worst computation complexity. If you use a uh indexing structure,", "1028.18-1031.369": "it would be OK times N squared", "1032.64-1051.209": "um And which is quite bad as you can imagine as a square uh is uh running slow. And so lay a detection as well as clustering can be quite slow. K here is the dimensionality of the feature space and N is the number of objects and number of flows.", "1051.51-1071.339": "The pro of such an index based approach is it scales well with the number of the dimensions. So that's, that's the problem we had in the statistics based methods um that it didn't scale on all of the features and the dimensions, it would basically run in the curse of dimensionality", "1071.68-1093.67": "or not even work at all. Um The condo is that an indexing structure takes a lot of time to be built. It has, it has high processing time and the construction itself also needs time to be created. And, and basically that's the problem you have with database systems as well. Once you use an index, it's an expensive choice.", "1097.219-1125.339": "A nested loop algorithm for outlier detection very similar to the nested loop join. If you know it, it divides the buffer where you load the data into two halves, the first and the second areas, it breaks the data into blocks and then feeds the two blocks into these areas. You directly compute the distance between each pair of the objects of the two areas inside the area. And between the area, you decide who is an outlier or not depending on the distance.", "1125.56-1138.699": "So the result of the distance, for example, Euclidean distance, it would tell you whether the higher, the more likely it's an outlier. You can basically also say, well, these are the candidates for being outliers and then you check them and refine them again,", "1139.25-1152.819": "it has the same computational complexity as the indexing based algorithm. The pros are you avoid the index structure construction and you try to mini minimize the io the input output levels.", "1153.739-1157.27": "It's a little bit of a better version than", "1157.89-1185.849": "um the indexing based one. So the example for the TED loop algorithm is here is the buffer, you have raarab, this is also the buffer buffer but now it's an area A and, and R AD and then you have the database or in this case, you can just call it the data. I don't want to call the data database. Uh Usually if you wouldn't use, like nowadays we use for data um data science.", "1186.06-1214.41": "Um UDFS use a defined function in Python and Jupiter notebooks, it's also quite common to load a database system. And once you have a database system, it's much easier to connect with indexing structures and allow for methods that are optimized by the optimizer inside the database system itself. So it might sometimes be very helpful to look into the database system that you're using and check if they have an inbuilt machine learning approach.", "1214.67-1233.56": "Um Something like this uh nested loop algorithm is likely um inbuilt into any database system. I'm not sure if Python can run it then, but I'm pretty sure it's possible to run it immediately if you look at the API for the database system.", "1234.76-1244.28": "But anyway, it's a database or, or data here just calls for the data in the databases and not for how the database does it in detail.", "1245.119-1274.319": "Um But let's assume that the data here is in the buffer, this is in the, in the storage and the normal data. And here you have four arrays and there are two. And this is basically the buffer is how you swap it in main memory. And this is basically how it, because it's an optimization technique for finding outliers, you, you manage to load all these areas one by one into the buffer and check it with the first area", "1274.63-1302.589": "in total. You have four reads and four comparisons. That's stage one. And in stage two, what you basically do, you now fix the last area, the D before you fix the A and then you check here, you iterate all the ones ABC without the D and then you have two reads because you don't need to read A anymore. A is done on ad in section from the beginning", "1303.77-1312.63": "in stage three, you do the same totally again to reads and two reads again. So this is basically how it optimizes", "1313.959-1338.15": "um last but not least we have the local outliers. This was uh created, you see the same here. So some outliers can be defined as global and some can be defined as local outliers. And this was, was semantically quite a big step in research to, to distinguish these two automatically. Again, here you have the global outlier 01 and here, the local outlier 02", "1338.369-1362.569": "um 02 would not normally be considered an outlier with regular distance based outlay detection. Also not necessarily with distribution based outlay detection. Um As all of these look at the global picture. So love or local outliers were the first time able to find these uh or two outliers here you see.", "1363.099-1378.64": "And the idea very similar to DB scan, it comes from from the same um research, each data object is assigned a local outlier factor. So these are the ones that are factored. You have one point P that you check, you have", "1378.8-1402.13": "this epsilon environment or you call it D MD max min points similar to DB scan here. This parameter uh defines how many are maximally allowed in this area for this thing to still be a uh an outlier. And then objects which are closer to dense clues receive a higher local outlier factor", "1402.14-1427.53": "than the ones who are farther away than very dense clusters of the density of this area, which is the density connectedness as you remember plays a crucial role. And the love varies according to the parameter min points, MIN PTS because this is basically how close it actually is in comparison to these others. So local outliers", "1427.8-1432.39": "uh in in in pictures, if you take a look at this,", "1432.939-1447.859": "if you plot the local outlier factor, right in 3d, these ones, these data points that are actual clusters year round and very clearly defined. And then the outliers, if it's for sure an outlet, the more global it is", "1448.469-1450.28": "the more higher", "1450.959-1474.17": "is the last local outlier factor. So this is just the outlier factor. So the global outlier factor is bigger. But if you would put the local outlier factor, usually something that is closer to these very dense cluster like this one would be very, very large. I think this is this one because this is global and it is a global outlier. This one is huge factor.", "1476.29-1477.109": "Mhm", "1478.04-1501.989": "OK. I have one more for you guys, the partition based one partition based clustering method is called birch. It uses a partition of points that uses a clustering algorithm. You compute the upper and lower bounds with these partitions and you identify candidate partitions and containing outliers and then you compute the n outliers from the points and candidate partitions.", "1502.5-1520.01": "So first we build your cluster which is a clustering approach, your cluster in partitions of non outliers, you prune these partitions that they do not contain outliers. And then you use these indexing or nested loop algorithms that we discussed before on the remaining data points.", "1520.29-1541.18": "Um As many data points are removed during pruning the efficiency of indexing and naive lag significantly improved. So this version, the partition based one using B which is first clustering, then outlay detection helps here a lot in terms of uh computation power.", "1543.28-1555.16": "If you have questions as this is an online um situation, please write me an email and um I'm seeing you next week. Thank you. Take care."}